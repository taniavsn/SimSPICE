{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from SproutDataset import SproutDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "from lightly.transforms import SimSiamTransform\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised search: run infidence on the backbone+projection head\n",
    "\n",
    "Feed the spectra thu the backbone, then backbone to proj head. Give 128 values. Encoding spectra to vector of size 128.\n",
    "End up with vector for each one of the spectra. \n",
    "\n",
    "Take those 128 values for all spectra, and pass them to Umap. Turn 128 values to (x,y) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese1DNet_backbone(nn.Module):\n",
    "    def __init__(self, output_dim=128):\n",
    "        super(Siamese1DNet_backbone, self).__init__()\n",
    "                \n",
    "        # Shared feature extraction network\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers for embeddings\n",
    "        self.fc1 = nn.Linear(7168, output_dim)  # keep the backbone complex enough\n",
    "    \n",
    "    def forward(self, x):\n",
    "    # Feature extraction\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x)))) #\n",
    "        # print('here')\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x)))) \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))   \n",
    "        return F.normalize(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\"\n",
    "datasetsingle = SproutDataset(dataset_path=dataset_path, augmentation_type='single')\n",
    "dataloader = DataLoader(\n",
    "            datasetsingle,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(pl.LightningModule):\n",
    "    def __init__(self, output_dim=2, backbone_output_dim=128, hidden_layer_dim=64):\n",
    "        super().__init__()\n",
    "        self.backbone = Siamese1DNet_backbone(output_dim=backbone_output_dim)\n",
    "\n",
    "        # projection head: map data representations into a space that facilitates comparison and learning\n",
    "        self.projection_head = SimSiamProjectionHead(backbone_output_dim, hidden_layer_dim, output_dim) \n",
    "\n",
    "        # prediction head: produce the final output from the learned features\n",
    "        self.prediction_head = SimSiamPredictionHead(output_dim, hidden_layer_dim, output_dim)\n",
    "        self.criterion =  NTXentLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        f = self.backbone(x)\n",
    "        z = self.projection_head(f)  # z are the embeddings, meaning the data represented in a lower dimension space.\n",
    "        p = self.prediction_head(z)\n",
    "        return z, p\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (x0, x1) = batch\n",
    "        # print('\\n', x0.shape, x1.shape)\n",
    "        z0, p0 = self.forward(x0)\n",
    "        z1, p1 = self.forward(x1)\n",
    "        loss = self.criterion(z0, z1)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optim = torch.optim.SGD(self.parameters(), lr=0.06)\n",
    "        return optim\n",
    "\n",
    "\n",
    "model = SimSiam(output_dim=2)\n",
    "dataset = datasetsingle\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"SimSiam_FullDataset_Training_2D_Contrastive\", log_model=True)\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, devices=1, accelerator=accelerator, logger=wandb_logger)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"C:\\\\Users\\\\tania\\\\Documents\\\\CU Boulder\\\\CU Fall 2024\\\\ASEN 6337\\Individual project\\SPICE_DeepLearning\\SimSiam_FullDataset_Training_2D_Contrastive\\piqf6v8d\\checkpoints\\epoch=4-step=18300.ckpt\"\n",
    "loaded_model = SimSiam.load_from_checkpoint(checkpoint)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_none = SproutDataset(dataset_path=\"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\", augmentation_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for i in tqdm.tqdm(range (dataset_none.__len__())):\n",
    "        spec = dataset_none.__getitem__(i).unsqueeze(0)\n",
    "        # Move tensor to the same device as the model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        loaded_model = loaded_model.to(device)\n",
    "        spec = spec.to(device)\n",
    "\n",
    "        outputs.append(loaded_model(spec)[0].cpu().numpy())\n",
    "\n",
    "## index same as in dataloader\n",
    "# Can stack it as a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = np.stack(outputs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(121)\n",
    "scatter = plt.scatter(stacked_outputs[:, 0], stacked_outputs[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('Output Dimension to 2D')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.subplot(122)\n",
    "plt.hist2d(stacked_outputs[:, 0], stacked_outputs[:, 1], bins=50, vmax=690)\n",
    "plt.grid(True)\n",
    "plt.title('Density histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist2d(stacked_outputs[:, 0], stacked_outputs[:, 1], bins=50, vmax=690)\n",
    "plt.grid(True)\n",
    "plt.title('Density histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper2D = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42).fit(stacked_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No cosine distance supported by HDBscan: L2 normalize and euclidian distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_L2 = stacked_outputs / np.linalg.norm(stacked_outputs, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=15, metric='euclidean')\n",
    "clusterer.fit(embedding_L2)\n",
    "labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full dataset\n",
    "plt.scatter(embedding_L2[:, 0], embedding_L2[:, 1], c=labels, cmap='Accent', s=10)\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.title('HDBSCAN Clustering - Contrastive loss\\n2-dimensional output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inverse_mapping_functions import WAVELENGTHS_ARRAY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_n_random_spectra_cluster(labels, stacked_outputs, chosen_cluster, dataset, nbr_items=5,\n",
    "                                  dataset_path=\"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train.nc\"):\n",
    "    '''\n",
    "    Plots nbr_item belonging to a given cluster.\n",
    "    dataset: SproutDataset object, with augmentation_type set to None.\n",
    "    '''\n",
    "    labels_reshaped = labels.reshape(-1, 1)\n",
    "    print('labels shape: ', labels_reshaped.shape)\n",
    "    outputs_with_labels = np.hstack((stacked_outputs, labels_reshaped))\n",
    "    idxes = np.where(outputs_with_labels[:,-1] == chosen_cluster)[0]\n",
    "    random_spectra_idx = np.random.choice(idxes, size=nbr_items, replace=False)\n",
    "\n",
    "    for i in random_spectra_idx:\n",
    "        plt.figure(figsize=[12,4], tight_layout=True)\n",
    "        item = dataset.__getitem__(i)\n",
    "        plt.plot(WAVELENGTHS_ARRAY, item[0])\n",
    "        plt.title(f'Item number #{i}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_random_spectra_cluster(labels, stacked_outputs, 1, dataset_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunraster.instr.spice import read_spice_l2_fits   \n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_item_map(item_nbr, dataset, plot=False, data_dir='C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\data_L2\\\\', key='Ne VIII 770 (Merged)',\n",
    "                 croplatbottom=725, croplattop=115, title=' '):\n",
    "    filename = str(dataset.isel(index=item_nbr)['filename'].data)\n",
    "    print(filename)\n",
    "    i,j = (dataset.isel(index=item_nbr)['x-index'].data, dataset.isel(index=item_nbr)['y-index'].data)\n",
    "    exposure = read_spice_l2_fits(data_dir+filename, memmap=False)\n",
    "    cube = exposure[key][0,:,croplattop:croplatbottom,:].data\n",
    "    plt.imshow(cube[20, :, :], aspect=1/4, cmap='gist_heat', vmax=np.nanquantile(cube[20, :, :], 0.999))\n",
    "    print(cube.shape)\n",
    "    if plot:\n",
    "        plt.plot(j,i, color='red', marker='+', label=str(item_nbr))\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "\n",
    "datasetmini = xr.open_dataset(\"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\")\n",
    "labels_1file = labels.reshape(610,192)\n",
    "\n",
    "plt.imshow(labels_1file, cmap='Accent', aspect=1/4)\n",
    "plt.colorbar()\n",
    "plt.title('Clustered map - 2022-02-20')\n",
    "plt.show()\n",
    "map_item_map(80000, datasetmini, title='2022-02-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
