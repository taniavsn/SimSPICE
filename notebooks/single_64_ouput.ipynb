{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean file with modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(parent_dir)\n",
    "parent_dir1 = os.path.abspath(os.path.join(parent_dir, '..'))\n",
    "sys.path.append(parent_dir1)\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from simspice.data.SproutDataset import SproutDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hdbscan\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "import simspice.utils.inverse_mapping_functions as imf\n",
    "# import simspice.models.Siamese_Architecture as SA\n",
    "# import simspice.models.Siamese_Architecture_Transformer as SA\n",
    "# import simspice.models.Siamese_Architecture_Resnet as SA\n",
    "import simspice.models.SimCLR_Architecture_Resnet as SA\n",
    "import wandb\n",
    "#import umap.umap_ as umap\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "plt.rcParams['image.origin'] = 'lower'\n",
    "\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "simspice = \"/d0/tvaresano/SimSPICE/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = simspice+\"spectra_train.nc\"\n",
    "dataset = SproutDataset(dataset_path=dataset_path, augmentation_type='single', csv_files=simspice+'L2_names.csv',\n",
    "                                           log_space=False, normalize_intensity=False)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 'resnet_SimCLR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [2,3]\n",
      "\n",
      "  | Name            | Type                 | Params | Mode \n",
      "-----------------------------------------------------------------\n",
      "0 | backbone        | ResNet1D             | 3.9 M  | train\n",
      "1 | projection_head | SimCLRProjectionHead | 25.0 K | train\n",
      "2 | criterion       | NTXentLoss           | 0      | train\n",
      "-----------------------------------------------------------------\n",
      "3.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.9 M     Total params\n",
      "15.738    Total estimated model params size (MB)\n",
      "74        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1635/2269 [12:17<04:46,  2.22it/s, v_num=lv4d]"
     ]
    }
   ],
   "source": [
    "model = SA.SimCLR(output_dim=64, backbone_output_dim=128, hidden_layer_dim=128)\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"runs_single_augmentation\", name=f\"fullTrained_outdim64_{id}_{datetime.today().strftime('%Y-%m-%d')}\", log_model=True)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=20, devices=1, accelerator=accelerator, logger=wandb_logger)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If loading from previous checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = simspice+\"\\\\notebooks\\\\FullDataset_64_doubleAug_normalized_spec\\\\k81c85sl\\\\checkpoints\\\\epoch=4-step=9075.ckpt\"\n",
    "# model = SA.SimSiam.load_from_checkpoint(checkpoint)  # Continue epochs\n",
    "\n",
    "# accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "# wandb_logger = WandbLogger(project=\"FullDataset_64_doubleAug_normalized_spec\", log_model=True)\n",
    "# trainer = pl.Trainer(max_epochs=10, devices=1, accelerator=accelerator, logger=wandb_logger)\n",
    "# trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 116160/116160 [07:27<00:00, 259.37it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = simspice+\"/spectra_Feb2023.nc\"\n",
    "dataset_none = SproutDataset(dataset_path=dataset_path, augmentation_type=None, csv_files=simspice+'L2_names.csv',\n",
    "                                           log_space=False, normalize_intensity=False)\n",
    "# checkpoint = simspice+'notebooks/runs_single_augmentation/vjewcesi/checkpoints/epoch=16-step=154275.ckpt'\n",
    "# outputs = SA.run_model(checkpoint, dataset_none)\n",
    "# model = SA.SimSiam.load_from_checkpoint(checkpoint)\n",
    "model.eval()\n",
    "outputs = []\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for i in tqdm.tqdm(range (dataset_none.__len__())):\n",
    "        spec = dataset_none.__getitem__(i).unsqueeze(0)\n",
    "        # Move tensor to the same device as the model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = model.to(device)\n",
    "        spec = spec.to(device)\n",
    "\n",
    "        outputs.append(model(spec)[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = \"C:\\\\Users\\\\tania\\Documents\\CU Boulder\\CU Alpine\\models_ckpts\\single_epoch=4-step=45750.ckpt\"\n",
    "# dataset_none = SproutDataset(dataset_path=dataset_path, augmentation_type=None)\n",
    "# outputs = SA.run_model(checkpoint, dataset_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = np.stack(outputs).squeeze()\n",
    "stacked_outputs.shape\n",
    "np.save(simspice+f'notebooks/jobs/model_outputs/stacked_outputs_single64_full_{id}.npy', stacked_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = np.load(simspice+f'notebooks/jobs/model_outputs/stacked_outputs_single64_full_{id}.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [10, 20, 30]:\n",
    "    for y in tqdm.tqdm([5, 10, 15]):\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=x, min_samples=y, metric='euclidean') # <=> cosine?\n",
    "        if stacked_outputs.shape[0] == 0:\n",
    "            print(f\"‚ö†Ô∏è No samples for x={x}, y={y}, skipping...\")\n",
    "            continue\n",
    "        clusterer.fit(stacked_outputs)\n",
    "        labels = clusterer.labels_\n",
    "        np.save(simspice+f'notebooks/jobs/clustering/Fulltrained_single32_minclus{x}_minsamp{y}.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "plt.figure(figsize=(15,12), tight_layout=True)\n",
    "for x in [10, 20, 30]:\n",
    "    for y in [5, 10, 15]:\n",
    "        c+=1\n",
    "        plt.subplot(4,3,c)\n",
    "        labels = np.load(simspice+f'notebooks/jobs/clustering/Fulltrained_single32_minclus{x}_minsamp{y}.npy')\n",
    "        imf.map_clusters(labels, dataset_path=simspice+'spectra_Feb2023.nc', selected_clusters=None)\n",
    "        plt.title(f\"min_cluster = {x}\\nmin_samples = {y}\")\n",
    "plt.suptitle('Feb23_Fulltrained_single32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simspice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
