{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import lightning as L\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from brokenaxes import brokenaxes\n",
    "import warnings \n",
    "import tqdm\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SproutDataset import SproutDataset\n",
    "from Augmentation import Augmentation\n",
    "from Sprout_ML import Sprout_ML, interpolate_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "from lightly.transforms import SimSiamTransform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests classes and first run of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetNone = SproutDataset(augmentation_type=None)\n",
    "datasetsingle = SproutDataset(augmentation_type='single')\n",
    "datasetdouble = SproutDataset(augmentation_type='double')\n",
    "item = datasetNone.__getitem__(75)  ## or item = datasetNone(75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetmini = xr.open_dataset(\"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(datasetmini.isel(index=75)['filename'].data)\n",
    "print(datasetmini.isel(index=760)['x-index'].data, datasetmini.isel(index=75)['y-index'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunraster.instr.spice import read_spice_l2_fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_item_map(item_nbr, dataset, plot=False, data_dir='C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\data_L2\\\\', key='Ne VIII 770 (Merged)',croplatbottom=725, croplattop=115):\n",
    "    filename = str(dataset.isel(index=item_nbr)['filename'].data)\n",
    "    i,j = (dataset.isel(index=item_nbr)['x-index'].data, dataset.isel(index=item_nbr)['y-index'].data)\n",
    "    exposure = read_spice_l2_fits(data_dir+filename, memmap=False)\n",
    "    cube = exposure[key][0,:,croplattop:croplatbottom,:].data\n",
    "    if plot:\n",
    "        plt.imshow(cube[20, :, :], aspect=1/4, cmap='gist_heat', vmax=np.nanquantile(cube[20, :, :], 0.999))\n",
    "        plt.plot(i, j, color='yellow', marker='+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(datasetmini.isel()['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_item_map(8999, datasetmini, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\"\n",
    "dataset_path = \"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\"\n",
    "datasetsingle = SproutDataset(dataset_path=dataset_path, augmentation_type='single')\n",
    "dataloader = DataLoader(\n",
    "            datasetsingle,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True)\n",
    "print(len(datasetsingle.__getitem__(65)), datasetsingle.__getitem__(65)[0].shape, datasetsingle.__getitem__(65)[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call lighlty (but assumes an image but we have 1D signals)\n",
    "# Check what architectures are popular for 1D data --> CNN? Physionet 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try 1 : CNN 1D\n",
    "https://github.com/harryjdavies/Python1D_CNNs/blob/master/CCN1D_pytorch_activity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"siamese-1st_try\", entity=\"tava8993-boulder\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese1DNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese1DNet, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Shared feature extraction network\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers for embeddings\n",
    "        self.fc1 = nn.Linear(7168, 128)   ## remove those? Can also keep\n",
    "        self.fc2 = nn.Linear(128, 64)  \n",
    "        self.fc3 = nn.Linear(64, 2)      # Output constrained to size 2\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # Final layer to constrain output to size 2\n",
    "        return F.normalize(x, dim=1)\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function: NT-Xent Loss. Commonly used in self-supervised learning methods like SimCLR. It measures the cosine similarity between embeddings.\n",
    "class NTXentLoss(nn.Module):\n",
    "    def __init__(self, temperature=0.5):\n",
    "        super(NTXentLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.similarity = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        # Normalize embeddings\n",
    "        z_i = F.normalize(z_i, dim=1)\n",
    "        z_j = F.normalize(z_j, dim=1)\n",
    "        \n",
    "        # Compute similarity\n",
    "        positives = self.similarity(z_i, z_j)\n",
    "        negatives = torch.mm(z_i, z_j.T)  # All pairs\n",
    "        \n",
    "        # Remove self-comparisons\n",
    "        batch_size = z_i.size(0)\n",
    "        mask = torch.eye(batch_size, device=z_i.device).bool()\n",
    "        negatives.masked_fill_(mask, -float('inf'))\n",
    "        \n",
    "        # Compute logits\n",
    "        logits = torch.cat([positives.unsqueeze(1), negatives], dim=1)\n",
    "        logits /= self.temperature\n",
    "        \n",
    "        # Labels for positives\n",
    "        labels = torch.zeros(batch_size, dtype=torch.long, device=z_i.device)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# model = Siamese1DNet().to(device)\n",
    "# criterion = NTXentLoss(temperature=0.5)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# wandb.watch(model, log=\"all\", log_freq=10)\n",
    "# for epoch in range(NUM_EPOCHS):\n",
    "#     model.train()  # Set model to training mode\n",
    "#     epoch_loss = 0.0\n",
    "    \n",
    "#     for batch in dataloader:\n",
    "#         # Assuming the dataloader returns (augmentation1, augmentation2)\n",
    "#         input1, input2 = batch\n",
    "#         input1 = input1.unsqueeze(1)  # Add channel dimension: [batch_size, 1, length]\n",
    "#         input2 = input2.unsqueeze(1)\n",
    "#         input1, input2 = input1.to(device).float(), input2.to(device).float()  # Move to device\n",
    "        \n",
    "#         # Forward pass\n",
    "#         output1, output2 = model(input1, input2)\n",
    "        \n",
    "#         # Compute loss\n",
    "#         loss = criterion(output1, output2)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward() ## compute gradients and update weights\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         epoch_loss += loss.item()\n",
    "#     avg_loss = epoch_loss / len(dataloader)\n",
    "#     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "#     wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "# wandb.save(\"siamese-1st_try.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Siamese1DNet().to(device)\n",
    "criterion = NTXentLoss(temperature=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "outputs_collected = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Assuming the dataloader returns (augmentation1, augmentation2)\n",
    "        input1, input2 = batch\n",
    "        input1 = input1.unsqueeze(1)  # Add channel dimension: [batch_size, 1, length]\n",
    "        input2 = input2.unsqueeze(1)\n",
    "        input1, input2 = input1.to(device).float(), input2.to(device).float()  # Move to device\n",
    "        \n",
    "        # Forward pass\n",
    "        output1, output2 = model(input1, input2)\n",
    "\n",
    "        # Collect outputs during training (optional)\n",
    "        if epoch == NUM_EPOCHS - 1:  # Collect only for the last epoch\n",
    "            outputs_collected.append((output1.detach().cpu(), output2.detach().cpu()))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output1, output2)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #compute gradients and update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "    wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"siamese-1st_try.pth\")\n",
    "\n",
    "# Combine outputs collected during the last epoch\n",
    "output1_all, output2_all = zip(*outputs_collected)\n",
    "output1_all = torch.cat(output1_all)\n",
    "output2_all = torch.cat(output2_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy\n",
    "output1_np = output1_all.numpy()\n",
    "output2_np = output2_all.numpy()\n",
    "combined_outputs = np.vstack((output1_np, output2_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(output1_np, output2_np, marker='.')\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(combined_outputs[:,0], combined_outputs[:,1], marker='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_similarity(output1, output2):\n",
    "    similarity = F.cosine_similarity(output1, output2)\n",
    "    return similarity\n",
    "\n",
    "# Example: Analyze a batch\n",
    "for batch in dataloader:\n",
    "    input1, input2 = batch\n",
    "    input1, input2 = input1.to(device).float(), input2.to(device).float()\n",
    "    output1, output2 = model(input1.unsqueeze(1), input2.unsqueeze(1))\n",
    "    \n",
    "    similarity = compute_similarity(output1, output2)\n",
    "    print(\"Similarity Scores:\", similarity)\n",
    "    plt.hist(similarity.detach().cpu().numpy(), bins=20, alpha=0.7)\n",
    "    plt.title(\"Similarity Distribution\")\n",
    "    plt.xlabel(\"Similarity\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pairs(input1, input2, similarity_scores):\n",
    "    num_pairs = min(len(input1), 5)  # Visualize 5 pairs\n",
    "    input1, input2 = input1.cpu().numpy(), input2.cpu().numpy()\n",
    "    \n",
    "    fig, axs = plt.subplots(num_pairs, 2, figsize=(8, num_pairs * 3))\n",
    "    for i in range(num_pairs):\n",
    "        axs[i, 0].plot(input1[i], label=f\"Input 1, Score={similarity_scores[i]:.2f}\")\n",
    "        axs[i, 0].legend()\n",
    "        axs[i, 1].plot(input2[i], label=\"Input 2\")\n",
    "        axs[i, 1].legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Visualize a batch\n",
    "for batch in dataloader:\n",
    "    input1, input2 = batch\n",
    "    input1, input2 = input1.to(device).float(), input2.to(device).float()\n",
    "    output1, output2 = model(input1.unsqueeze(1), input2.unsqueeze(1))\n",
    "    similarity = compute_similarity(output1, output2)\n",
    "    \n",
    "    visualize_pairs(input1, input2, similarity.detach().cpu().numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetsingle.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese1DNet_64dim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese1DNet_64dim, self).__init__()\n",
    "        \n",
    "        \n",
    "        # Shared feature extraction network\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.conv2 = nn.Conv1d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # Fully connected layers for embeddings\n",
    "        self.fc1 = nn.Linear(7168, 128)   ## remove those? Can also keep\n",
    "        self.fc2 = nn.Linear(128, 64)      # Output constrained to size 2\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        # Feature extraction\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return F.normalize(x, dim=1)\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Siamese1DNet_64dim().to(device)\n",
    "criterion = NTXentLoss(temperature=0.5)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "outputs_collected = []\n",
    "NUM_EPOCHS = 10\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        # Assuming the dataloader returns (augmentation1, augmentation2)\n",
    "        input1, input2 = batch\n",
    "        input1 = input1.unsqueeze(1)  # Add channel dimension: [batch_size, 1, length]\n",
    "        input2 = input2.unsqueeze(1)\n",
    "        input1, input2 = input1.to(device).float(), input2.to(device).float()  # Move to device\n",
    "        \n",
    "        # Forward pass\n",
    "        output1, output2 = model(input1, input2)\n",
    "\n",
    "        # Collect outputs during training (optional)\n",
    "        if epoch == NUM_EPOCHS - 1:  # Collect only for the last epoch\n",
    "            outputs_collected.append((output1.detach().cpu(), output2.detach().cpu()))\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output1, output2)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() #compute gradients and update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "    wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"siamese-1st_try.pth\")\n",
    "\n",
    "# Combine outputs collected during the last epoch\n",
    "output1_all, output2_all = zip(*outputs_collected)\n",
    "output1_all = torch.cat(output1_all)\n",
    "output2_all = torch.cat(output2_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1_np_64 = output1_all.numpy()\n",
    "output2_np_64 = output2_all.numpy()\n",
    "output1_np_64.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(output1_np_64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1_np_64.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the 2D UMAP embedding AND 2D HISTOGRAM\n",
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(121)\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('UMAP Projection to 2D')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.subplot(122)\n",
    "plt.hist2d(embedding[:, 0], embedding[:, 1], bins=50)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=30, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(output1_np_64)\n",
    "\n",
    "# Plotting the 2D UMAP embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('UMAP Projection to 2D')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=30, min_dist=0.01, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(output1_np_64)\n",
    "\n",
    "# Plotting the 2D UMAP embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('UMAP Projection to 2D')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=50, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(output1_np_64)\n",
    "\n",
    "# Plotting the 2D UMAP embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('UMAP Projection to 2D')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=100, min_dist=0.1, n_components=2, random_state=42)\n",
    "embedding = reducer.fit_transform(output1_np_64)\n",
    "\n",
    "# Plotting the 2D UMAP embedding\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(embedding[:, 0], embedding[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('UMAP Projection to 2D')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised search: run infidence on the backbone+projection head\n",
    "\n",
    "Feed the spectra thu the backbone, then backbone to proj head. Give 128 values. Encoding spectra to vector of size 128.\n",
    "End up with vector for each one of the spectra. \n",
    "\n",
    "Take those 128 values for all spectra, and pass them to Umap. Turn 128 values to (x,y) coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use (H)DBScan to cluster the 2D output\n",
    "\n",
    "Go through files, each pixel will be numbered according to the cluster. Vizualize the files (rasters) with a categorial colomap."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once clustering is done, assess index to cluster number and then can make maps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
