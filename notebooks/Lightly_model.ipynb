{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from SproutDataset import SproutDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from lightly.loss import NegativeCosineSimilarity\n",
    "from lightly.models.modules import SimSiamPredictionHead, SimSiamProjectionHead\n",
    "from lightly.transforms import SimSiamTransform\n",
    "import torch\n",
    "from Siamese_Architecture import Siamese1DNet_backbone, SimSiam\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import umap.umap_ as umap\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import sklearn.metrics as metrics\n",
    "from SproutDataset import map_item_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised search: run infidence on the backbone+projection head\n",
    "\n",
    "Feed the spectra thu the backbone, then backbone to proj head. Give 128 values. Encoding spectra to vector of size 128.\n",
    "End up with vector for each one of the spectra. \n",
    "\n",
    "Take those 128 values for all spectra, and pass them to Umap. Turn 128 values to (x,y) coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train.nc\"\n",
    "dataset_path_mini = \"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\"\n",
    "datasetsingle = SproutDataset(dataset_path=dataset_path_mini, augmentation_type='single')\n",
    "dataloader = DataLoader(\n",
    "            datasetsingle,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    if isinstance(batch, list):  # If batch is a list\n",
    "        batch = torch.stack(batch)  # Stack it into a tensor\n",
    "    print(batch.shape)  # Should now show [32, 451]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimSiam()\n",
    "dataset = datasetsingle\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"SimSiam_Training_128_contrastive\", log_model=True)\n",
    "accelerator = \"gpu\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(max_epochs=5, devices=1, accelerator=accelerator, logger=wandb_logger)\n",
    "trainer.fit(model=model, train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"C:\\\\Users\\\\tania\\\\Documents\\\\CU Boulder\\\\CU Fall 2024\\\\ASEN 6337\\\\Individual project\\\\SPICE_DeepLearning\\\\SimSiam_Training_128_contrastive\\\\50a3p00v\\\\checkpoints\\epoch=4-step=18300.ckpt\"\n",
    "loaded_model = SimSiam.load_from_checkpoint(checkpoint)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_none = SproutDataset(dataset_path=dataset_path, augmentation_type=None)\n",
    "dataset_none_mini = SproutDataset(dataset_path=dataset_path_mini, augmentation_type=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "with torch.no_grad():  # Disable gradient computation for inference\n",
    "    for i in tqdm.tqdm(range (dataset_none_mini.__len__())):\n",
    "        spec = dataset_none_mini.__getitem__(i).unsqueeze(0) \n",
    "\n",
    "        # Move tensor to the same device as the model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        loaded_model = loaded_model.to(device)\n",
    "        spec = spec.to(device)\n",
    "\n",
    "        outputs.append(loaded_model(spec)[0].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = np.stack(outputs).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs_L2 = stacked_outputs / np.linalg.norm(stacked_outputs, ord=2)\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=100, min_samples=15, metric='euclidean')\n",
    "clusterer.fit(stacked_outputs_L2)\n",
    "\n",
    "labels = clusterer.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunraster.instr.spice import read_spice_l2_fits   \n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_item_map(item_nbr, dataset, plot=False, data_dir='C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\data_L2\\\\', key='Ne VIII 770 (Merged)',croplatbottom=725, croplattop=115):\n",
    "    filename = str(dataset.isel(index=item_nbr)['filename'].data)\n",
    "    i,j = (dataset.isel(index=item_nbr)['x-index'].data, dataset.isel(index=item_nbr)['y-index'].data)\n",
    "    exposure = read_spice_l2_fits(data_dir+filename, memmap=False)\n",
    "    cube = exposure[key][0,:,croplattop:croplatbottom,:].data\n",
    "    if plot:\n",
    "        plt.imshow(cube[20, :, :], aspect=1/4, cmap='gist_heat', vmax=np.nanquantile(cube[20, :, :], 0.999))\n",
    "        print(cube.shape)\n",
    "        plt.plot(j,i, color='red', marker='+')\n",
    "\n",
    "datasetmini = xr.open_dataset(\"C:\\\\Users\\\\tania\\\\Documents\\\\SPICE\\\\SPROUTS\\\\spectra_train_mini.nc\")\n",
    "labels_1file = labels.reshape(610,192)\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.subplot(211)\n",
    "plt.imshow(labels_1file, cmap='Accent', aspect=1/4)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "map_item_map(80000, datasetmini, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, n_components=2, random_state=42)\n",
    "projected_data = reducer.fit_transform(stacked_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "plt.subplot(121)\n",
    "scatter = plt.scatter(projected_data[:, 0], projected_data[:, 1], cmap='Spectral', s=1)\n",
    "plt.title('UMAP Projection to 2D')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.subplot(122)\n",
    "plt.hist2d(projected_data[:, 0], projected_data[:, 1], bins=50)\n",
    "plt.grid(True)\n",
    "plt.title('Density histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.Spectral\n",
    "cmap.set_under('white')\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(projected_data[:, 0], projected_data[:, 1], c=labels, cmap='Spectral', s=10, alpha=0.5)\n",
    "cbar = plt.colorbar(label='Cluster')\n",
    "\n",
    "plt.title('HDBSCAN Clustering - Contrastive loss\\n2-dimensional output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = labels != -1\n",
    "filtered_data = projected_data[mask]\n",
    "filtered_labels = labels[mask]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15, 10))\n",
    "scatter = plt.scatter(\n",
    "    filtered_data[:, 0],\n",
    "    filtered_data[:, 1],\n",
    "    c=filtered_labels,\n",
    "    cmap='Spectral',\n",
    "    s=10\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title('HDBSCAN Clustering - Contrastive Loss\\n2-dimensional output (excluding -1)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
